{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Evaluaci√≥n de Base de Datos en 3 Pasos\n",
    "\n",
    "Este notebook permite evaluar una base de datos de manera r√°pida y directa.\n",
    "\n",
    "## Requisitos previos\n",
    "\n",
    "1. Dependencias instaladas: `pip install -r requirements_fase3.txt`\n",
    "2. Modelo spaCy descargado: `python -m spacy download es_core_news_sm`\n",
    "3. Configuraci√≥n `.env` con `OPENAI_API_KEY` (opcional para diagn√≥stico IA)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema listo\n",
      "OpenAI configurado: Si\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Agregar ruta del proyecto\n",
    "sys.path.append('..')\n",
    "\n",
    "from fase3_evaluator.integration import run_parallel_analysis, get_analysis_summary\n",
    "from fase3_evaluator.agent import generate_diagnosis\n",
    "from config import settings\n",
    "\n",
    "print(\"Sistema listo\")\n",
    "print(f\"OpenAI configurado: {'Si' if settings.OPENAI_API_KEY else 'No (solo analisis tecnico)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1b: Funci√≥n Helper para Conversi√≥n de Tipos NumPy\n",
    "\n",
    "Esta funci√≥n es necesaria para evitar errores de serializaci√≥n JSON con tipos de NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funcion de conversion cargada\n"
     ]
    }
   ],
   "source": [
    "def convert_numpy_types(obj):\n",
    "    \"\"\"\n",
    "    Convierte tipos NumPy/Pandas a tipos nativos Python para JSON serialization.\n",
    "    \n",
    "    Args:\n",
    "        obj: Objeto a convertir\n",
    "        \n",
    "    Returns:\n",
    "        Objeto convertido a tipos nativos Python\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "print(\"Funcion de conversion cargada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Carga de Datos\n",
    "\n",
    "### Opci√≥n A: Archivo CSV propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCION A: Carga de archivo CSV propio\n",
    "# Modificar esta ruta con la ubicaci√≥n del archivo\n",
    "ruta_archivo = \"../data/mi_archivo.csv\"\n",
    "\n",
    "# Descomentar la siguiente l√≠nea para cargar el archivo:\n",
    "# df = pd.read_csv(ruta_archivo)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opci√≥n B: Dataset de Ejemplo (para pruebas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 50 registros, 9 columnas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fecha_evento</th>\n",
       "      <th>edad</th>\n",
       "      <th>sexo</th>\n",
       "      <th>metodo</th>\n",
       "      <th>municipio</th>\n",
       "      <th>latitud</th>\n",
       "      <th>longitud</th>\n",
       "      <th>tipo_evento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>25.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ahorcamiento</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>40.4168</td>\n",
       "      <td>-3.7038</td>\n",
       "      <td>consumado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>F</td>\n",
       "      <td>intoxicacion</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>39.4699</td>\n",
       "      <td>-0.3763</td>\n",
       "      <td>intento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-01-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>41.3851</td>\n",
       "      <td>2.1734</td>\n",
       "      <td>consumado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>45.0</td>\n",
       "      <td>M</td>\n",
       "      <td>arma de fuego</td>\n",
       "      <td>Sevilla</td>\n",
       "      <td>37.3891</td>\n",
       "      <td>-5.9845</td>\n",
       "      <td>intento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-02-04</td>\n",
       "      <td>52.0</td>\n",
       "      <td>F</td>\n",
       "      <td>ahorcamiento</td>\n",
       "      <td>Bilbao</td>\n",
       "      <td>43.2627</td>\n",
       "      <td>-2.9253</td>\n",
       "      <td>consumado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id fecha_evento  edad sexo         metodo  municipio  latitud  longitud  \\\n",
       "0   1   2024-01-07  25.0    M   ahorcamiento     Madrid  40.4168   -3.7038   \n",
       "1   2   2024-01-14  30.0    F   intoxicacion   Valencia  39.4699   -0.3763   \n",
       "2   3   2024-01-21   NaN    M           None  Barcelona  41.3851    2.1734   \n",
       "3   4   2024-01-28  45.0    M  arma de fuego    Sevilla  37.3891   -5.9845   \n",
       "4   5   2024-02-04  52.0    F   ahorcamiento     Bilbao  43.2627   -2.9253   \n",
       "\n",
       "  tipo_evento  \n",
       "0   consumado  \n",
       "1     intento  \n",
       "2   consumado  \n",
       "3     intento  \n",
       "4   consumado  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPCION B: Dataset de ejemplo para probar el sistema\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, 51),\n",
    "    'fecha_evento': pd.date_range('2024-01-01', periods=50, freq='W'),\n",
    "    'edad': [25, 30, None, 45, 52, 38, 60, 28, 35, 42] * 5,\n",
    "    'sexo': ['M', 'F', 'M', 'M', 'F'] * 10,\n",
    "    'metodo': ['ahorcamiento', 'intoxicacion', None, 'arma de fuego', 'ahorcamiento'] * 10,\n",
    "    'municipio': ['Madrid', 'Valencia', 'Barcelona', 'Sevilla', 'Bilbao'] * 10,\n",
    "    'latitud': [40.4168, 39.4699, 41.3851, 37.3891, 43.2627] * 10,\n",
    "    'longitud': [-3.7038, -0.3763, 2.1734, -5.9845, -2.9253] * 10,\n",
    "    'tipo_evento': ['consumado', 'intento', 'consumado', 'intento', 'consumado'] * 10\n",
    "})\n",
    "\n",
    "print(f\"Dataset cargado: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Ejecuci√≥n del An√°lisis Completo\n",
    "\n",
    "### 3.1 An√°lisis T√©cnico (6 dimensiones en paralelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fase3_evaluator.integration.orchestrator:Iniciando an√°lisis paralelo de 'mi_dataset.csv' con 50 registros y 9 columnas\n",
      "INFO:fase3_evaluator.integration.orchestrator:Ejecutando analizadores en paralelo...\n",
      "INFO:fase3_evaluator.integration.orchestrator:‚úì geospatial completado\n",
      "INFO:fase3_evaluator.integration.orchestrator:‚úì completeness completado\n",
      "INFO:fase3_evaluator.integration.orchestrator:‚úì typology completado\n",
      "INFO:fase3_evaluator.integration.orchestrator:‚úì anonymization completado\n",
      "INFO:fase3_evaluator.integration.orchestrator:‚úì semantic completado\n",
      "INFO:fase3_evaluator.integration.orchestrator:‚úì ml_readiness completado\n",
      "WARNING:fase3_evaluator.integration.orchestrator:PII detectada en 1 columnas - anonimizando...\n",
      "INFO:fase3_evaluator.integration.orchestrator:Anonimizaci√≥n completada y validada\n",
      "INFO:fase3_evaluator.integration.orchestrator:Consolidando resultados...\n",
      "WARNING:fase3_evaluator.integration.orchestrator:Advertencia en validaci√≥n Pydantic: 1 validation error for ConsolidatedAnalysis\n",
      "ml.model_suggestions.0.models\n",
      "  Input should be a valid string [type=string_type, input_value=['Logistic Regression', '...ndom Forest', 'XGBoost'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "INFO:fase3_evaluator.integration.orchestrator:An√°lisis completado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando analisis tecnico...\n",
      "\n",
      "\n",
      "Analisis completado\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejecutando analisis tecnico...\\n\")\n",
    "\n",
    "# Ejecutar an√°lisis con anonimizaci√≥n autom√°tica si se detecta PII\n",
    "consolidated_json, df_anonymized, anonymization_report = run_parallel_analysis(\n",
    "    df,\n",
    "    filename=\"mi_dataset.csv\",\n",
    "    auto_anonymize=True\n",
    ")\n",
    "\n",
    "print(\"\\nAnalisis completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESUMEN EJECUTIVO\n",
      "======================================================================\n",
      "\n",
      "Dataset:\n",
      "   50 registros x 9 columnas\n",
      "\n",
      "Calidad de los Datos:\n",
      "   Completitud     [###################.] 97/100 (OK)\n",
      "   Tipologia       [###################.] 96/100 (OK)\n",
      "   Semantica       [####################] 100/100 (OK)\n",
      "   Geoespacial     [####################] 100/100 (OK)\n",
      "   Ml_viabilidad   [##########..........] 50/100 (REVISAR)\n",
      "\n",
      "   SCORE GENERAL   [#################...] 88/100\n",
      "\n",
      "Issues Criticos:\n",
      "   [!] PII detectada: MODERADO\n",
      "       Anonimizada automaticamente\n",
      "\n",
      "Capacidades Analiticas:\n",
      "   [+] Analisis geoespacial y mapas\n",
      "   [+] Clustering (deteccion de patrones espaciales)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generar resumen ejecutivo\n",
    "summary = get_analysis_summary(consolidated_json)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESUMEN EJECUTIVO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Informaci√≥n del dataset\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"   {summary['dataset']['rows']:,} registros x {summary['dataset']['columns']} columnas\")\n",
    "\n",
    "# Scores de calidad\n",
    "print(f\"\\nCalidad de los Datos:\")\n",
    "for metric, score in summary['scores'].items():\n",
    "    bar = \"#\" * int(score/5) + \".\" * (20 - int(score/5))\n",
    "    status = \"OK\" if score >= 70 else \"REVISAR\" if score >= 50 else \"CRITICO\"\n",
    "    print(f\"   {metric.capitalize():15s} [{bar}] {score:.0f}/100 ({status})\")\n",
    "\n",
    "# Score general\n",
    "overall = summary['overall_score']\n",
    "bar_overall = \"#\" * int(overall/5) + \".\" * (20 - int(overall/5))\n",
    "print(f\"\\n   {'SCORE GENERAL':15s} [{bar_overall}] {overall:.0f}/100\")\n",
    "\n",
    "# Issues cr√≠ticos\n",
    "print(f\"\\nIssues Criticos:\")\n",
    "has_issues = False\n",
    "if summary['critical_issues']['pii_detected']:\n",
    "    print(f\"   [!] PII detectada: {summary['critical_issues']['pii_risk_level'].upper()}\")\n",
    "    if anonymization_report:\n",
    "        print(f\"       Anonimizada automaticamente\")\n",
    "    has_issues = True\n",
    "if summary['critical_issues']['completitud_critica']:\n",
    "    print(f\"   [!] Completitud critica en campos clave\")\n",
    "    has_issues = True\n",
    "if summary['critical_issues']['leakage_risks'] > 0:\n",
    "    print(f\"   [!] {summary['critical_issues']['leakage_risks']} riesgos de data leakage\")\n",
    "    has_issues = True\n",
    "if not has_issues:\n",
    "    print(f\"   No se detectaron issues criticos\")\n",
    "\n",
    "# Capacidades anal√≠ticas\n",
    "print(f\"\\nCapacidades Analiticas:\")\n",
    "if summary['capabilities']['geocodable']:\n",
    "    print(f\"   [+] Analisis geoespacial y mapas\")\n",
    "if summary['capabilities']['clustering_feasible']:\n",
    "    print(f\"   [+] Clustering (deteccion de patrones espaciales)\")\n",
    "if summary['capabilities']['ml_viable']:\n",
    "    print(f\"   [+] Machine Learning (modelos predictivos)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Diagn√≥stico con IA (Opcional)\n",
    "\n",
    "Requiere `OPENAI_API_KEY` configurada en `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fase3_evaluator.agent.interpreter:Generando diagn√≥stico con OpenAI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando diagnostico interpretativo con IA...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:fase3_evaluator.agent.interpreter:Diagn√≥stico generado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNOSTICO GENERADO POR IA\n",
      "======================================================================\n",
      "Modelo: gpt-4o | Tokens: 6224\n",
      "======================================================================\n",
      "\n",
      "## 1. DIAGN√ìSTICO GENERAL\n",
      "\n",
      "El dataset analizado contiene 50 registros con 9 columnas, cubriendo eventos relacionados con el suicidio. La calidad general de los datos es excelente, con un puntaje de completitud del 96.67%. Sin embargo, se identificaron algunas √°reas cr√≠ticas que requieren atenci√≥n, como la inconsistencia en la tipolog√≠a de datos en la columna \"sexo\" y la presencia de informaci√≥n potencialmente identificable (PII) en la columna \"id\".\n",
      "\n",
      "Los hallazgos m√°s importantes incluyen: \n",
      "1. Una inconsistencia significativa en la columna \"sexo\", donde el 40% de los valores no coinciden con el tipo de dato esperado.\n",
      "2. La columna \"metodo\" presenta un 20% de datos faltantes, lo que limita el an√°lisis detallado de los m√©todos utilizados.\n",
      "3. La presencia de PII en la columna \"id\", lo que implica un riesgo moderado de re-identificaci√≥n.\n",
      "\n",
      "## 2. AN√ÅLISIS QUE S√ç SE PUEDEN REALIZAR\n",
      "\n",
      "‚úÖ **An√°lisis Descriptivo de Distribuciones**\n",
      "üìä Permite entender la distribuci√≥n de casos por edad, sexo, m√©todo y localidad.\n",
      "üéØ Apoya la identificaci√≥n de grupos demogr√°ficos en riesgo y √°reas geogr√°ficas con mayor incidencia.\n",
      "\n",
      "‚úÖ **An√°lisis Geoespacial**\n",
      "üìä Utiliza coordenadas para identificar concentraciones geogr√°ficas de eventos.\n",
      "üéØ Facilita la focalizaci√≥n de recursos en √°reas con alta incidencia de suicidios.\n",
      "\n",
      "‚úÖ **An√°lisis de Correlaciones**\n",
      "üìä Detecta asociaciones entre variables como edad y m√©todo.\n",
      "üéØ Informa estrategias de prevenci√≥n al identificar patrones demogr√°ficos asociados a m√©todos espec√≠ficos.\n",
      "\n",
      "## 3. AN√ÅLISIS QUE NO SE PUEDEN REALIZAR (Y POR QU√â)\n",
      "\n",
      "‚ùå **Modelos de Machine Learning Predictivo**\n",
      "   Raz√≥n: Insuficientes muestras (50 < 100 requeridas) y problemas de inconsistencia en la columna \"sexo\".\n",
      "   Para desbloquearlo: Aumentar el tama√±o del dataset y estandarizar los datos inconsistentes.\n",
      "\n",
      "‚ùå **Predicci√≥n de M√©todo de Suicidio**\n",
      "   Raz√≥n: La columna \"metodo\" tiene un 20% de datos faltantes.\n",
      "   Para desbloquearlo: Completar los datos faltantes y estandarizar los m√©todos seg√∫n c√≥digos reconocidos.\n",
      "\n",
      "## 4. HALLAZGOS DESCRIPTIVOS CLAVE\n",
      "\n",
      "- **Distribuci√≥n por Edad**: La mayor√≠a de los casos se concentran entre los 25 y 60 a√±os, con una media de 39.44 a√±os.\n",
      "- **Distribuci√≥n por Sexo**: 60% de los casos son hombres y 40% mujeres.\n",
      "- **M√©todos Utilizados**: Ahorcamiento es el m√©todo m√°s com√∫n (40%), seguido por intoxicaci√≥n y arma de fuego (20% cada uno).\n",
      "- **Localidad**: Los casos est√°n distribuidos equitativamente entre cinco municipios principales.\n",
      "\n",
      "## 5. CORRELACIONES Y PATRONES IDENTIFICADOS\n",
      "\n",
      "Se identific√≥ una asociaci√≥n entre la edad y el m√©todo utilizado, sugiriendo que ciertos m√©todos de alta letalidad podr√≠an estar m√°s presentes en grupos etarios espec√≠ficos. Esto no implica causalidad, pero sugiere la necesidad de investigar m√°s a fondo el acceso a ciertos m√©todos en diferentes grupos de edad.\n",
      "\n",
      "## 6. RIESGOS √âTICOS Y DE PRIVACIDAD\n",
      "\n",
      "‚ö†Ô∏è Se detect√≥ PII en la columna \"id\", con un riesgo moderado de re-identificaci√≥n. Aunque se aplic√≥ anonimizaci√≥n autom√°tica, se recomienda eliminar esta columna si no es esencial para el an√°lisis, para cumplir con las normativas de privacidad como el RGPD.\n",
      "\n",
      "## 7. RECOMENDACIONES PRIORIZADAS\n",
      "\n",
      "1. [Prioridad CR√çTICA] **Eliminar o anonimizar la columna \"id\"** para mitigar riesgos de re-identificaci√≥n. Impacto: Alto. Dificultad: F√°cil.\n",
      "   \n",
      "2. [Prioridad ALTA] **Estandarizar los datos en la columna \"sexo\"** para resolver inconsistencias. Impacto: Alto. Dificultad: Media.\n",
      "\n",
      "3. [Prioridad MEDIA] **Completar los datos faltantes en la columna \"metodo\"** para mejorar la calidad del an√°lisis de m√©todos. Impacto: Medio. Dificultad: Media.\n",
      "\n",
      "4. [Prioridad MEDIA] **Ampliar el tama√±o del dataset** para habilitar an√°lisis de ML en el futuro. Impacto: Alto. Dificultad: Dif√≠cil.\n",
      "\n",
      "5. [Prioridad BAJA] **Revisar y ajustar las categor√≠as de m√©todos** seg√∫n est√°ndares internacionales para mejorar la comparabilidad. Impacto: Medio. Dificultad: Media.\n",
      "\n",
      "En conclusi√≥n, aunque el dataset presenta algunos desaf√≠os, sigue siendo una herramienta valiosa para entender y prevenir el suicidio. Con las acciones recomendadas, se puede mejorar significativamente su utilidad anal√≠tica y su cumplimiento con las normativas de privacidad.\n"
     ]
    }
   ],
   "source": [
    "if settings.OPENAI_API_KEY:\n",
    "    print(\"Generando diagnostico interpretativo con IA...\\n\")\n",
    "    \n",
    "    try:\n",
    "        diagnosis_result = generate_diagnosis(\n",
    "            consolidated_json,\n",
    "            df_anonymized,\n",
    "            include_sample=True\n",
    "        )\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"DIAGNOSTICO GENERADO POR IA\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Modelo: {diagnosis_result['model_used']} | Tokens: {diagnosis_result['tokens_used']}\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\\n\" + diagnosis_result['diagnosis'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar diagnostico: {str(e)}\")\n",
    "else:\n",
    "    print(\"OpenAI no configurado - solo analisis tecnico disponible\")\n",
    "    print(\"Para habilitar diagnostico IA: configurar OPENAI_API_KEY en .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Exploraci√≥n de Resultados Detallados (Opcional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 An√°lisis de Completitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALISIS DE COMPLETITUD\n",
      "Score: 96.7/100\n",
      "Valores faltantes: 3.3%\n",
      "\n",
      "Top columnas con mas valores faltantes:\n",
      "  - metodo: 20.0%\n",
      "  - edad: 10.0%\n",
      "  - id: 0.0%\n",
      "  - fecha_evento: 0.0%\n",
      "  - sexo: 0.0%\n"
     ]
    }
   ],
   "source": [
    "completeness = consolidated_json['completitud']\n",
    "\n",
    "print(\"ANALISIS DE COMPLETITUD\")\n",
    "print(f\"Score: {completeness['evaluation']['score']:.1f}/100\")\n",
    "print(f\"Valores faltantes: {completeness['summary']['missing_percentage']:.1f}%\")\n",
    "\n",
    "if completeness['critical_fields_missing']:\n",
    "    print(f\"\\nCampos criticos faltantes: {', '.join(completeness['critical_fields_missing'])}\")\n",
    "\n",
    "print(\"\\nTop columnas con mas valores faltantes:\")\n",
    "for col_info in completeness['top_missing_columns'][:5]:\n",
    "    print(f\"  - {col_info['column']}: {col_info['missing_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 An√°lisis ML Readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALISIS ML READINESS\n",
      "Viable: No\n",
      "Confianza: nula\n",
      "Score: 50.0/100\n",
      "\n",
      "Target detectado: tipo_evento\n",
      "Features usables: 6\n",
      "\n",
      "Modelos sugeridos:\n",
      "\n",
      "  clasificacion_binaria:\n",
      "    Modelos: Logistic Regression, Random Forest, XGBoost\n",
      "    Razon: Dataset peque√±o - modelos interpretables recomendados\n"
     ]
    }
   ],
   "source": [
    "ml = consolidated_json['ml']\n",
    "\n",
    "print(\"ANALISIS ML READINESS\")\n",
    "print(f\"Viable: {'Si' if ml['ml_viability']['viable'] else 'No'}\")\n",
    "print(f\"Confianza: {ml['ml_viability']['confidence']}\")\n",
    "print(f\"Score: {ml['ml_viability']['score']:.1f}/100\")\n",
    "\n",
    "print(f\"\\nTarget detectado: {ml['target_column']}\")\n",
    "print(f\"Features usables: {ml['features_analysis']['usable_features']}\")\n",
    "\n",
    "if ml['model_suggestions']:\n",
    "    print(\"\\nModelos sugeridos:\")\n",
    "    for suggestion in ml['model_suggestions'][:3]:\n",
    "        print(f\"\\n  {suggestion['task']}:\")\n",
    "        print(f\"    Modelos: {', '.join(suggestion['models'])}\")\n",
    "        print(f\"    Razon: {suggestion['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 An√°lisis de Privacidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALISIS DE PRIVACIDAD Y ANONIMIZACION\n",
      "PII detectada: Si\n",
      "Risk score: 3.3/10\n",
      "Nivel: MODERADO\n",
      "\n",
      "Entidades detectadas:\n",
      "  - ID_NUMBER en 'id': 50 instancias\n",
      "\n",
      "Anonimizacion aplicada:\n",
      "  - id: removed\n"
     ]
    }
   ],
   "source": [
    "anon = consolidated_json['anonimizacion']\n",
    "\n",
    "print(\"ANALISIS DE PRIVACIDAD Y ANONIMIZACION\")\n",
    "print(f\"PII detectada: {'Si' if anon['summary']['pii_detected'] else 'No'}\")\n",
    "\n",
    "if anon['summary']['pii_detected']:\n",
    "    print(f\"Risk score: {anon['summary']['risk_score']:.1f}/10\")\n",
    "    print(f\"Nivel: {anon['summary']['risk_level'].upper()}\")\n",
    "    \n",
    "    print(\"\\nEntidades detectadas:\")\n",
    "    for entity in anon['entities_found'][:5]:\n",
    "        print(f\"  - {entity['type']} en '{entity['column']}': {entity['count']} instancias\")\n",
    "    \n",
    "    if anonymization_report:\n",
    "        print(\"\\nAnonimizacion aplicada:\")\n",
    "        for col, method in list(anonymization_report['transformation_details'].items())[:5]:\n",
    "            print(f\"  - {col}: {method}\")\n",
    "else:\n",
    "    print(\"No se detecto informacion personal identificable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5: Exportaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON guardado: ../data/outputs/analisis_20251125_122639.json\n",
      "CSV guardado: ../data/outputs/datos_anonimizados_20251125_122639.csv\n",
      "Diagnostico guardado: ../data/outputs/diagnostico_20251125_122639.md\n",
      "\n",
      "Todos los archivos guardados en: /Users/reinerfuentesferrada/ONLINE_DS_THEBRIDGE_Rei/Proyecto ML/cuidar-ia/cuidar_ia_fase3 4/notebooks/../data/outputs\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta de outputs\n",
    "output_dir = Path('../data/outputs')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. JSON completo - Convertir tipos NumPy antes de guardar\n",
    "json_path = output_dir / f'analisis_{timestamp}.json'\n",
    "consolidated_json_clean = convert_numpy_types(consolidated_json)\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(consolidated_json_clean, f, indent=2, ensure_ascii=False)\n",
    "print(f\"JSON guardado: {json_path}\")\n",
    "\n",
    "# 2. Datos anonimizados\n",
    "csv_path = output_dir / f'datos_anonimizados_{timestamp}.csv'\n",
    "df_anonymized.to_csv(csv_path, index=False)\n",
    "print(f\"CSV guardado: {csv_path}\")\n",
    "\n",
    "# 3. Diagn√≥stico (si existe)\n",
    "if 'diagnosis_result' in locals():\n",
    "    md_path = output_dir / f'diagnostico_{timestamp}.md'\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Diagnostico Automatico\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(diagnosis_result['diagnosis'])\n",
    "    print(f\"Diagnostico guardado: {md_path}\")\n",
    "\n",
    "print(f\"\\nTodos los archivos guardados en: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## An√°lisis Completado\n",
    "\n",
    "He evaluado la base de datos completa en pocos minutos.\n",
    "\n",
    "### Pr√≥ximos pasos:\n",
    "\n",
    "1. **Notebooks avanzados:**\n",
    "   - `01_test_analizadores.ipynb` - Prueba individual de cada analizador\n",
    "   - `02_flujo_completo.ipynb` - Flujo detallado con explicaciones\n",
    "\n",
    "2. **Personalizaci√≥n:**\n",
    "   - Ajustar umbrales en `config/settings.py`\n",
    "   - Personalizar prompts en `fase3_evaluator/agent/prompts.py`\n",
    "\n",
    "3. **Integraci√≥n:**\n",
    "   - Usar en Streamlit: `pages/5_Evaluador_Bases_Datos.py`\n",
    "   - Integrar en scripts propios\n",
    "\n",
    "### Soporte:\n",
    "- Email: reinefuentes7@gmail.com\n",
    "- Documentaci√≥n: `../README.md`\n",
    "- Instalaci√≥n: `../docs/INSTALACION.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cuidar (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
